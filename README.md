# MTech-AI-Capstone-Project
National University of Singapore - Institute of System Science - Masters of Technology in Artificial Intelligence Systems - Capstone Project
<br>
# Problem Statement
The persistent misalignment between question complexity and perceived difficulty presents a fundamental challenge in AI-driven educational technologies, where subjective human judgments frequently diverge from objective question characteristics. 
<br>
<p><i>“How can we improve the AI system’s ability to generate questions with complexity levels that consistently align with human expectations of difficulty?”</i></p>

# Challenges
Achieving proper alignment between AI understanding of question complexity and human perception of difficulty faces several interrelated challenges.
<img width="2880" height="2310" alt="Chellenges in Measuring Complexity" src="https://github.com/user-attachments/assets/097b0611-63e0-4d80-ae08-3f664c5fe1b8" />


This is because "difficulty" is:
- A Fluid Concept
- Malleable
- Hard to Define and Measure

Hence, to date, a universal standard for defining and measuring question difficulty does not exist.

# Proposed Solution - The Cerebro Index Framework
Introducing the Cerebro Index (CI), a framework that integrates educational theory, cognitive science, and technical metrics into a unified system to quantify the complexity of a question.
<img width="2970" height="1800" alt="Cerebro Index Framework - visual selection" src="https://github.com/user-attachments/assets/b6f3aa52-65b0-487e-a13f-4ef04550fc61" />
<br>
When given a text question as input, the Cerebro Index Framework outputs a complexity score (CI Score) that quantifies and represents the question’s inherent complexity. CI Scores are in a continuous range from 0.0 to 10.0 (inclusive).
The CI framework addresses limitations in existing methods, such as subjective human judgment, rigid rule-based systems, and opaque machine learning models, by combining educational theory with scalable, interpretable metrics.  


# Human Validation of the CI Framework
<b>Research Questions</b>
1.	RQ1: Do CI scores positively correlate with human-perceived question complexity rankings?
2.	RQ2: Do CI complexity categories align with those assigned by human evaluators?




<img width="602" height="425" alt="RQ1 Correlation" src="https://github.com/user-attachments/assets/3920906a-8008-4998-91e9-c8fe64010665" />
